{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================================\n",
    "#\n",
    "# TESTS SUR LES RUN.PY\n",
    "#\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import full_preprocessing\n",
    "\n",
    "X_train, Y_train, Y_train_oh, ids, X_test = full_preprocessing()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c74ce3aff152>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrun\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerate_submission_from_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\JUST_THE_NECESSARY\\Github\\ML2\\run.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrun_fct\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\JUST_THE_NECESSARY\\Github\\ML2\\LSTM_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from run import generate_submission_from_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nimport pandas as pd\\nfrom our_functionsv3 import *\\n\\nimport tensorflow.keras as keras\\nfrom tensorflow import set_random_seed\\nset_random_seed(0)\\nfrom keras.models import Model\\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\\nfrom keras.layers.embeddings import Embedding\\nfrom keras.preprocessing import sequence\\nfrom keras.initializers import glorot_uniform\\nfrom keras.callbacks import EarlyStopping\\nfrom keras import regularizers\\nimport time\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from our_functionsv3 import *\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "#\n",
    "# OTHER TEST FOR THE RUN.py\n",
    "#\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import full_preprocessing\n",
    "from run_fct import read_glove_vecs_only_alpha, get_max_length, sentences_to_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52700 words were not in the dictionary\n",
      "2698 words were not in the dictionary\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, Y_train_oh, ids, X_test = full_preprocessing()\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs_only_alpha('dictionnary/glove.twitter.27B.200d.txt')\n",
    "\n",
    "max_length = get_max_length(X_train)\n",
    "\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, max_length)\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiley_LSTM_improved(input_shape, word_to_vec_map, word_to_index, dropout_rate, hidden_layers, reg, l1_lambda, l2_lambda):\n",
    "    \n",
    "    if reg=='l2':\n",
    "        regularizer = regularizers.l2(l2_lambda)\n",
    "    elif reg=='l1':\n",
    "        regularizer = regularizers.l1(l1_lambda)\n",
    "    else:\n",
    "        regularizer = None\n",
    "        \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    X = Conv2D(32, kernel_size=(3, 3), activation='relu') (embeddings)\n",
    "    X = MaxPooling2D(pool_size=(2, 2)) (X)\n",
    "    X = Conv2D(64, (3, 3), activation='relu') (X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(128, activation='relu')\n",
    "    X = Dropout(0.5)(X)    \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    '''\n",
    "    X = LSTM(32, return_sequences = True, return_state = False, kernel_regularizer=regularizer)(X)\n",
    "    X = Dropout(0.5)(X) \n",
    "    X = LSTM(32, return_sequences = True, return_state = False, kernel_regularizer=regularizer)(X)\n",
    "    X = Dropout(0.5)(X)   \n",
    "    X = LSTM(32, return_sequences = False, return_state = False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    '''\n",
    "    \n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 2-dimensional vectors.\n",
    "    X = Dense(2)(X)\n",
    "    \n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "\n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs = sentence_indices, outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_model_improved(X_train_indices, Y_train_oh, word_to_vec_map, word_to_index, max_length, summary = False, dropout_rate = 0.25, batch_size = 128, \n",
    "                   epochs = 50, loss ='categorical_crossentropy', optimizer ='adam', hidden_layers = 1, \n",
    "                            reg = '', l1_lambda = 0, l2_lambda = 0):\n",
    "    \n",
    "    model = smiley_LSTM_improved((max_length,), word_to_vec_map, word_to_index, dropout_rate, hidden_layers, reg, l1_lambda, l2_lambda)\n",
    "    \n",
    "    if summary:\n",
    "        model.summary()\n",
    "        \n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=3, verbose=1, mode='auto')\n",
    "    callbacks_list = [earlystop]\n",
    "    \n",
    "    start = time.time()\n",
    "    history_lstm = model.fit(X_train_indices, Y_train_oh, epochs = 50, callbacks=callbacks_list, batch_size = 32, validation_split = 0.1, shuffle=True)\n",
    "    end = time.time()\n",
    "    print(\"Model took {} seconds (which is {} minutes or {} hours) to train\".format((end - start), (end - start)/60, (end - start)/3600))\n",
    "    \n",
    "    return history_lstm, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f4fbb2af4239>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plots_file import *\n",
    "from our_functionsv3 import *\n",
    "\n",
    "np.random.seed(0)\n",
    "import tensorflow.keras as keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.callbacks import EarlyStopping\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "l2_lambda = [0]\n",
    "for l2 in l2_lambda:\n",
    "    history, model = complete_model_improved(X_train_indices, Y_train_oh, word_to_vec_map, word_to_index, max_length, summary = True, dropout_rate = 50, batch_size = 128, epochs = 50, loss ='categorical_crossentropy', \n",
    "                                             optimizer = 'adam', hidden_layers = 0, reg = None, l1_lambda = 0, l2_lambda = 0)\n",
    "    label = predict_lstm(model, X_test_indices)\n",
    "    path = 'submissions/submission_model_cnn.csv'\n",
    "    #create_csv_submission(ids, label, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "#\n",
    "#  EN DESSOUS : des tests sur la fonction preprocessing \n",
    "#\n",
    "# ============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(X_train[0:4], \"\\n\")\\nprint(Y_train[0:4], \"\\n\")\\nprint(Y_train_oh[0:4], \"\\n\")\\nprint(ids[0:4], \"\\n\")\\nprint(X_test[0:4], \"\\n\")\\n\\nprint(type(Y_train_oh))\\nprint(type(Y_train))\\nprint(type(X_train))\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JE TESTE MA FONCTION full_preprocessing ICI\n",
    "\"\"\"\n",
    "print(X_train[0:4], \"\\n\")\n",
    "print(Y_train[0:4], \"\\n\")\n",
    "print(Y_train_oh[0:4], \"\\n\")\n",
    "print(ids[0:4], \"\\n\")\n",
    "print(X_test[0:4], \"\\n\")\n",
    "\n",
    "print(type(Y_train_oh))\n",
    "print(type(Y_train))\n",
    "print(type(X_train))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "#============================================================================\n",
    "####################### LA JE FAIS STEP BY STEP #############################\n",
    "#============================================================================\n",
    "#============================================================================\n",
    "\n",
    "from preprocessing import *\n",
    "from our_functionsv3 import convert_to_one_hot, get_test_data\n",
    "\n",
    "path_pos = \"twitter-datasets/train_pos.txt\"\n",
    "path_neg = \"twitter-datasets/train_neg.txt\"\n",
    "path_test = \"twitter-datasets/test_data.txt\"\n",
    "\n",
    "train_pos = read_data(path_pos)\n",
    "train_neg = read_data(path_neg)\n",
    "\n",
    "pos = pd.DataFrame(train_pos, columns=[\"tweet\"])\n",
    "neg = pd.DataFrame(train_neg, columns=[\"tweet\"])\n",
    "\n",
    "test = read_data(path_test)\n",
    "test_pd = pd.DataFrame(test, columns=[\"tweet\"])\n",
    "\n",
    "train_pos_preprocessed = pre_process_tweets(pos)\n",
    "train_neg_preprocessed = pre_process_tweets(neg)\n",
    "\n",
    "################# SECOND PART ############\n",
    "\n",
    "X_pos = list(train_pos_preprocessed.tweet)\n",
    "X_neg = list(train_neg_preprocessed.tweet)\n",
    "X_train = X_pos + X_neg\n",
    "\n",
    "Y_pos = np.ones(len(X_pos), dtype = int)\n",
    "Y_neg = np.zeros(len(X_neg), dtype = int)\n",
    "Y_train = np.concatenate((Y_pos, Y_neg), axis = -1)\n",
    "\n",
    "X_train, Y_train = shuffle(X_train, Y_train, random_state=52) #shuffle our training set\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C=2)\n",
    "\n",
    "ids, _ = get_test_data(path_test)\n",
    "\n",
    "\n",
    "test_preprocessed = pre_process_tweets(test_pd)\n",
    "X_test = list(test_preprocessed.tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sea doo pro sea scooter sports with the portable seascootersave air stay longer in the water and url',\n",
       " 'user shucks well i work all week so now i ca not come cheer you on oh and put those batteries in your calculator',\n",
       " 'i cant stay away from bug thats my baby',\n",
       " 'user no lol im perfectly fine and not contagious anymore lmao',\n",
       " 'whenever i fall asleep watching the tv i always wake up with a headache',\n",
       " 'user he needs to get rid of that thing it scares me lol but he do not need a car either he needs drivers ed again',\n",
       " 'its whatever in a terrible mood',\n",
       " 'yesss rt user user thanks jordan i love you and i m gon na call you later']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################## JE CHECK SI MA FONCTION A L'AIR OK !!!\n",
    "X_train[0:4]\n",
    "Y_train[0:4]\n",
    "Y_train_oh[0:4]\n",
    "X_test[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_pos_preprocessed)\n",
    "train_pos_preprocessed.tweet[0:4]\n",
    "type(train_pos_preprocessed.tweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user i dunno justin read my mention or not only justin and god knows about that but i hope you will follow me #believe',\n",
       " 'because your logic is so dumb i wo not even crop out your name or your photo tsk url',\n",
       " 'user just put casper in a box looved the battle #crakkbitch',\n",
       " 'user user thanks sir do not trip lil mama just keep doin ya thang',\n",
       " 'visiting my brother tmr is the bestest birthday gift eveerrr',\n",
       " 'user yay #lifecompleted tweet facebook me to let me know please',\n",
       " 'user #1dnextalbumtitle feel for you rollercoaster of life song cocept life #yolo becoming famous #followmeplz',\n",
       " 'workin hard or hardly workin rt user at hardee s with my future coworker user']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################### LINK BETWEEN PART 1 AND 2 ##################################\n",
    "X_pos_try = list(train_pos_preprocessed.tweet)\n",
    "X_pos_try[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### SECOND PART #####################################\n",
    "from our_functionsv3 import read_data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "pos = \"twitter-datasets/train_pos_tokenize_not_hash.txt\"\n",
    "neg = \"twitter-datasets/train_neg_tokenize_not_hash.txt\"\n",
    "\n",
    "test_path = \"twitter-datasets/test_preprocessed_tokenize_not_hash.txt\"\n",
    "\n",
    "X_pos = read_data(pos)\n",
    "X_neg = read_data(neg)\n",
    "X_train = X_pos + X_neg\n",
    "\n",
    "Y_pos = np.ones(len(X_pos), dtype = int)\n",
    "Y_neg = np.zeros(len(X_neg), dtype = int)\n",
    "Y_train = np.concatenate((Y_pos, Y_neg), axis = -1)\n",
    "\n",
    "X_train_shuffled, Y_train_shuffled = shuffle(X_train, Y_train, random_state=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user i dunno justin read my mention or not only justin and god knows about that but i hope you will follow me #believe',\n",
       " 'because your logic is so dumb i wo not even crop out your name or your photo tsk url',\n",
       " 'user just put casper in a box looved the battle #crakkbitch',\n",
       " 'user user thanks sir do not trip lil mama just keep doin ya thang']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pos[0:4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
