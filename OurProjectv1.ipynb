{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project outline\n",
    "\n",
    "1. Baseline model: words embeddings and sgd as given by the course\n",
    "\n",
    "2. Words embeddings with downloaded library of twitter\n",
    "\n",
    "3. Using LSTM after words embeddings part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from our_functionsv3 import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Baseline model: word embeddings and sgd with glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training tweets and the built GloVe word embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do as told in README.md of project on github. \n",
    "(put train_pos.txt and train_neg.txt in main directory therefore or adapt cooc.py and build_vocab.sh)\n",
    "Also downloaded file are of form 'train_neg.txt', and course given function take 'neg_train.txt. I changed but you might also need to adapt if full training set or something. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build_vocab.sh  --> outputs vocab.txt <br>\n",
    "cut_vocab.sh --> outputs vocab_cut.txt <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python pickle_vocab.py --> outputs **vocab.pkl**  <br>\n",
    "Dictionary for each word with {key=word: value=index}   <br>\n",
    "Len is (nb_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python cooc.py --> outputs cooc.pkl <br>\n",
    "Co-occurence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove_solution.py  -->  outputs **embeddings.npy** <br>\n",
    "Embedding of each words  <br>\n",
    "shape is (nb_words, embedding_dimension + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a feature representation of each training tweet (by averaging the word vectors over all words of the tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_embeddings_vecs('./build_dictionnary/embeddings_preprocessed.npy', './build_dictionnary/vocab_preprocessed.pkl')\n",
    "#word_to_index, index_to_word, word_to_vec_map = read_embeddings_vecs('./build_dictionnary/embeddings.npy', './build_dictionnary/vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Linear Classifier: (e.g. logistic regression or SVM) on constructed features (scikit or our code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, Y_train = get_data(pos = \"./datasets/train_pos.txt\", neg = \"./datasets/train_neg.txt\")\n",
    "X_train, Y_train = get_data(pos = \"./build_dictionnary/train_pos_preprocessed.txt\", neg = \"./build_dictionnary/train_neg_preprocessed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 0.39974847493253235\n",
      "Accuracy: 0.5948075984380085\n",
      "Epoch: 1 --- cost = 0.3996910444148535\n",
      "Accuracy: 0.5948279368695086\n",
      "Epoch: 2 --- cost = 0.3996909640855177\n",
      "Accuracy: 0.5948279368695086\n",
      "Epoch: 3 --- cost = 0.3996909639585408\n",
      "Accuracy: 0.5948279368695086\n",
      "Epoch: 4 --- cost = 0.39969096395833237\n",
      "Accuracy: 0.5948279368695086\n",
      "Epoch: 5 --- cost = 0.39969096395833204\n",
      "Accuracy: 0.5948279368695086\n",
      "Epoch: 6 --- cost = 0.39969096395833204\n",
      "Accuracy: 0.5948279368695086\n",
      "Epoch: 7 --- cost = 0.39969096395833204\n",
      "Accuracy: 0.5948279368695086\n",
      "Epoch: 8 --- cost = 0.39969096395833187\n",
      "Accuracy: 0.5948279368695086\n",
      "Epoch: 9 --- cost = 0.39969096395833187\n",
      "Accuracy: 0.5948279368695086\n",
      "Epoch: 10 --- cost = 0.39969096395833187\n",
      "Accuracy: 0.5948279368695086\n"
     ]
    }
   ],
   "source": [
    "W, b = model(X_train, Y_train, word_to_vec_map, learning_rate = 0.005, num_iterations = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 0.7540312558819057\n",
      "Accuracy: 0.605025\n",
      "Epoch: 1 --- cost = 0.8065586645816134\n",
      "Accuracy: 0.602945\n",
      "Epoch: 2 --- cost = 0.8338989825363046\n",
      "Accuracy: 0.60195\n",
      "Epoch: 3 --- cost = 0.8485781853896398\n",
      "Accuracy: 0.602075\n",
      "Epoch: 4 --- cost = 0.8566246850531992\n",
      "Accuracy: 0.601765\n"
     ]
    }
   ],
   "source": [
    "W, b = model(X_train, Y_train, word_to_vec_map, learning_rate = 0.0005, num_iterations = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 0.5962095792900589\n",
      "Accuracy: 0.5637355597136349\n",
      "Epoch: 1 --- cost = 0.5798622606444553\n",
      "Accuracy: 0.5794571672632607\n",
      "Epoch: 2 --- cost = 0.5666481251481478\n",
      "Accuracy: 0.5840790758216726\n",
      "Epoch: 3 --- cost = 0.5558981032811396\n",
      "Accuracy: 0.5840078913114221\n",
      "Epoch: 4 --- cost = 0.5470947685558374\n",
      "Accuracy: 0.5834689228766677\n"
     ]
    }
   ],
   "source": [
    "W, b = model(X_train, Y_train, word_to_vec_map, learning_rate = 0.0001, num_iterations = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 0.4883787659559774\n",
      "Accuracy: 0.5906941506671005\n",
      "Epoch: 1 --- cost = 0.4732093740317805\n",
      "Accuracy: 0.5894382525219655\n",
      "Epoch: 2 --- cost = 0.4699363120121361\n",
      "Accuracy: 0.5891941913439636\n",
      "Epoch: 3 --- cost = 0.4691640019815648\n",
      "Accuracy: 0.5892043605597136\n",
      "Epoch: 4 --- cost = 0.46897307107557407\n"
     ]
    }
   ],
   "source": [
    "W, b = model(X_train, Y_train, word_to_vec_map, learning_rate = 0.001, num_iterations = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction: Predict labels for all tweets in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, X_test = get_test_data(\"test_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = predict(X_train_shuffled, Y_train_shuffled, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_test(X_test, W, b, word_to_vec_map)\n",
    "y_pred[y_pred == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission / Evaluation: Submit your predictions to kaggle\n",
    "Your submission file for the 10â€™000 tweets must be of the form: tweet-id, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids, y_pred, 'submissions/submission_model_with_preprocessing_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
