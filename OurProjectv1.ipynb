{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project outline\n",
    "\n",
    "1. Baseline model: words embeddings and sgd as given by the course\n",
    "\n",
    "2. Words embeddings with downloaded library of twitter\n",
    "\n",
    "3. Using LSTM after words embeddings part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project2_data.project_text_classification.cooc import main\n",
    "import numpy as np\n",
    "from our_functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Baseline model: word embeddings and sgd with glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training tweets and the built GloVe word embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do as told in README.md of project on github. \n",
    "(put train_pos.txt and train_neg.txt in main directory therefore or adapt cooc.py and build_vocab.sh)\n",
    "Also downloaded file are of form 'train_neg.txt', and course given function take 'neg_train.txt. I changed but you might also need to adapt if full training set or something. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build_vocab.sh  --> outputs vocab.txt <br>\n",
    "cut_vocab.sh --> outputs vocab_cut.txt <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python pickle_vocab.py --> outputs **vocab.pkl**  <br>\n",
    "Dictionary for each word with {key=word: value=index}   <br>\n",
    "Len is (nb_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python cooc.py --> outputs cooc.pkl <br>\n",
    "Co-occurence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove_solution.py  -->  outputs **embeddings.npy** <br>\n",
    "Embedding of each words  <br>\n",
    "shape is (nb_words, embedding_dimension + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a feature representation of each training tweet (by averaging the word vectors over all words of the tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings_vecs(embeddings, vocabulary):\n",
    "    with open(vocabulary, 'rb') as voc:\n",
    "        vocab = pickle.load(voc)\n",
    "        \n",
    "    words_embeddings = np.load('embeddings.npy')      # (nb_words, embedding_dimension)\n",
    "        \n",
    "    words = []                          # on veut les mots que une fois\n",
    "    word_to_vec_map = {}  \n",
    "    \n",
    "    for word, idx in vocab.items():  \n",
    "        words.append(word)                 #only possible because dict is ordered\n",
    "        word_to_vec_map[word] = words_embeddings[idx, :]\n",
    "        \n",
    "    i = 1\n",
    "    words_to_index = {}\n",
    "    index_to_words = {}\n",
    "    for w in sorted(words):\n",
    "        words_to_index[w] = i\n",
    "        index_to_words[i] = w\n",
    "        i = i + 1\n",
    "    \n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')\n",
    "word_to_index, index_to_word, word_to_vec_map = read_embeddings_vecs('embeddings.npy', 'vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(word_to_vec_map) = 21261\n",
    "# len(vocab) = 21161"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Linear Classifier: (e.g. logistic regression or SVM) on constructed features (scikit or our code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model is:\n",
    "$$ z^{(i)} = W . avg^{(i)} + b$$\n",
    "$$ a^{(i)} = softmax(z^{(i)})$$\n",
    "$$ \\mathcal{L}^{(i)} = - \\sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data):\n",
    "    with open(data, \"r\") as file:\n",
    "        tweets = str()\n",
    "        for _, line in enumerate(file):\n",
    "            tweets += line\n",
    "        tweets = tweets.split('\\n')\n",
    "        del tweets[-1]\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = read_data(\"./datasets/train_pos.txt\")\n",
    "X_neg = read_data(\"./datasets/train_neg.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_pos + X_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pos = np.ones(len(X_pos), dtype = int)\n",
    "Y_neg = np.zeros(len(X_neg), dtype = int) #np.ones(len(X_neg)) * (-1)\n",
    "Y_train = np.concatenate((Y_pos, Y_neg), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train_shuffled, Y_train_shuffled = shuffle(X_train, Y_train, random_state=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    Y_hot = np.empty([len(Y), 2])\n",
    "    Y_hot[Y== 1] = [1, 0]\n",
    "    Y_hot[Y==-1] = [0, 1]\n",
    "    return Y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "    \"\"\"\n",
    "    Model to train word vector representations in numpy.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
    "    Y -- labels, numpy array of integers between -1 and 1, numpy-array of shape (m, 1)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 20-dimensional vector representation\n",
    "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    num_iterations -- number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(32)\n",
    "\n",
    "    # Define number of training examples\n",
    "    m = len(Y)                              # number of training examples\n",
    "    n_y = 2                                 # number of classes  \n",
    "    n_h = 20                                # dimensions of the embeddings vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, n_y)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations):                       # Loop over the number of iterations\n",
    "        for i in range(m):                                # Loop over the training examples\n",
    "        \n",
    "            # Average the word vectors of the words from the i'th training example\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "\n",
    "            # Forward propagate the avg through the softmax layer\n",
    "            z = W @ avg + b\n",
    "            a = softmax(z)\n",
    "            \n",
    "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
    "            cost = - np.sum(Y_oh[i]*np.log(a))\n",
    "            \n",
    "            # Compute gradients \n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        \n",
    "        if t % 10 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 0.7186076235364628\n",
      "Accuracy: 0.611075\n",
      "Epoch: 10 --- cost = 0.7192028050243952\n",
      "Accuracy: 0.611085\n",
      "Epoch: 20 --- cost = 0.719202805024395\n",
      "Accuracy: 0.611085\n"
     ]
    }
   ],
   "source": [
    "W, b = model(X_train_shuffled, Y_train_shuffled, word_to_vec_map, learning_rate = 0.005, num_iterations = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction: Predict labels for all tweets in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(data):\n",
    "    with open(data, \"r\") as file:\n",
    "        X_test = []\n",
    "        ids = []\n",
    "        for _, line in enumerate(file):\n",
    "            ids.append( line.split(',', 1)[0] )\n",
    "            X_test.append( \" \".join(line.split(',', 1)[1:] ) )\n",
    "    return ids, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, X_test = get_test_data(\"test_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<user> he needs to get rid of that thing ! it scares me lol but he don't need a car either . he needs drivers ed again .\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.611085\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train_shuffled, Y_train_shuffled, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_test(X_test, W, b, word_to_vec_map)\n",
    "y_pred[y_pred == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission / Evaluation: Submit your predictions to kaggle\n",
    "Your submission file for the 10â€™000 tweets must be of the form: tweet-id, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Function taken from helpers of project 1\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids, y_pred, 'submissions/submission_model_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
