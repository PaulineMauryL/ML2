{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing \n",
    "\n",
    "This notebook provides a function that removes unnecessary characters such as stop words or punctuation from the tweets received in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = pd.read_csv(\"datasets/train_pos.txt\", sep=\"\\n\", names = [\"tweet\"], error_bad_lines=False);\n",
    "train_neg = pd.read_csv(\"datasets/train_neg.txt\", sep=\"\\n\", names = [\"tweet\"], error_bad_lines=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; i dunno justin read my mention or not ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because your logic is so dumb , i won't even c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;user&gt; just put casper in a box !  looved the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; thanks sir &gt; &gt; don't trip lil ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting my brother tmr is the bestest birthda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  <user> i dunno justin read my mention or not ....\n",
       "1  because your logic is so dumb , i won't even c...\n",
       "2   <user> just put casper in a box !  looved the...\n",
       "3  <user> <user> thanks sir > > don't trip lil ma...\n",
       "4  visiting my brother tmr is the bestest birthda..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Obtain stop words\n",
    "\n",
    "In this case, the stop words are only limited to the english dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(get_stop_words('en'))         \n",
    "nltk_words = list(stopwords.words('english')) \n",
    "stop_words.extend(nltk_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the first 10 stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(stop_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tweet Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'user', '>', 'i', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', '.', 'only', 'justin', 'and', 'god', 'knows', 'about', 'that', ',', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', '#', 'believe', '15']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of the data (separation of the tweets into words or characters, e.g. \"<\", \"#\")\n",
    "# Removal of space charaters and set to lower casing\n",
    "words = [x.strip().lower() for x in nltk.word_tokenize(train_pos[\"tweet\"][0])]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', 'i', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', 'only', 'justin', 'and', 'god', 'knows', 'about', 'that', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', 'believe']\n"
     ]
    }
   ],
   "source": [
    "# Keep only the words that are only contains characters that are part of the alphabet\n",
    "words = [word for word in words if word.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', 'dunno', 'justin', 'read', 'mention', 'justin', 'god', 'knows', 'hope', 'follow', 'believe']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "output = [w for w in words if not w in stop_words]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user dunno justin read mention justin god knows hope follow believe\n"
     ]
    }
   ],
   "source": [
    "# Recombine the strings to form a sentence\n",
    "print(\" \".join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 PreProcessTweets function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(tweet):\n",
    "    \n",
    "    words = [x.strip().lower() for x in nltk.word_tokenize(tweet)]\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    output = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessTweets(data):\n",
    "    \n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: Preprocess(x))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_preprocessed = PreProcessTweets(train_pos)\n",
    "train_neg_preprocessed = PreProcessTweets(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_preprocessed.to_csv('train_pos_preprocessed.txt', header=None, index=False, sep='\\t')\n",
    "train_neg_preprocessed.to_csv('train_neg_preprocessed.txt', header=None, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
