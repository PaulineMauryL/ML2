{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing \n",
    "\n",
    "This notebook provides a function that removes unnecessary characters such as stop words or punctuation from the tweets received in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from our_functionsv3 import read_data\n",
    "import nltk\n",
    "#from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_data(\"twitter-datasets/test_data.txt\")\n",
    "test_pd = pd.DataFrame(test, columns=[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = read_data(\"twitter-datasets/train_pos.txt\")\n",
    "train_neg = read_data(\"twitter-datasets/train_neg.txt\")\n",
    "\n",
    "pos = pd.DataFrame(train_pos, columns=[\"tweet\"])\n",
    "neg = pd.DataFrame(train_neg, columns=[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_full = read_data(\"twitter-datasets/train_pos_full.txt\")\n",
    "train_neg_full = read_data(\"twitter-datasets/train_neg_full.txt\")\n",
    "\n",
    "pos_full = pd.DataFrame(train_pos_full, columns=[\"tweet\"])\n",
    "neg_full = pd.DataFrame(train_neg_full, columns=[\"tweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Obtain stop words\n",
    "\n",
    "In this case, the stop words are only limited to the english dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = list(get_stop_words('en'))         \n",
    "nltk_words = list(stopwords.words('english')) \n",
    "#stop_words.extend(nltk_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['allo', 'bidule', '#manger', 'don\\'t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [word for word in a if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['allo', 'bidule']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 PreProcessTweets function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    \n",
    "    words = [x.strip().lower() for x in nltk.word_tokenize(tweet)]\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    output = [w for w in words if not w in nltk_words]\n",
    "    \n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_tweets(data):\n",
    "    \n",
    "    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: preprocess(x))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessed = pre_process_tweets(test_pd)\n",
    "test_preprocessed.to_csv('twitter-datasets/test_preprocessed_to_use.txt', header=None, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_preprocessed = pre_process_tweets(pos)\n",
    "train_neg_preprocessed = pre_process_tweets(neg)\n",
    "\n",
    "train_pos_preprocessed.to_csv('twitter-datasets/train_pos_preprocessed_to_use.txt', header=None, index=False, sep='\\t')\n",
    "train_neg_preprocessed.to_csv('twitter-datasets/train_neg_preprocessed_to_use.txt', header=None, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_preprocessed_full = pre_process_tweets(pos_full)\n",
    "train_neg_preprocessed_full = pre_process_tweets(neg_full)\n",
    "\n",
    "train_pos_preprocessed_full.to_csv('twitter-datasets/train_pos_preprocessed_full.txt', header=None, index=False, sep='\\t')\n",
    "train_neg_preprocessed_full.to_csv('twitter-datasets/train_neg_preprocessed_full.txt', header=None, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
